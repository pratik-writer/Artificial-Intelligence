{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "\n",
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"zalando-research/fashionmnist\")\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "g-MFQjGFGBa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape,y_train.shape)\n",
        "print(x_test.shape,y_test.shape)\n",
        "print(x_val.shape,y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "QffI7sKEHuXL",
        "outputId": "ad15299c-65a0-4d55-8063-0ee580aae6c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28) (60000,)\n",
            "(10000, 28, 28) (10000,)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'x_val' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-2597641918>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'x_val' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_val,y_val)=(x_train[50000:],y_train[50000:])"
      ],
      "metadata": {
        "id": "JRk0jzBNbZuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_val.shape,y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-21UpqJbf3J",
        "outputId": "92fa1969-866d-4707-faa0-bbc630af42ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 28, 28) (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train[5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycEr5y9aeT-H",
        "outputId": "e026266e-816a-4679-84ee-a8a43666c279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define class labels for FashionMNIST\n",
        "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# Define the transform used for training/validation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load the dataset\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Get a batch of images\n",
        "data_iter = iter(train_loader)\n",
        "images, labels = next(data_iter)\n",
        "\n",
        "# Unnormalize images for display (convert from [-1, 1] to [0, 1])\n",
        "images = images * 0.5 + 0.5\n",
        "\n",
        "# Plot the images with their class names\n",
        "fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(16):\n",
        "    image = images[i].squeeze()  # remove channel dim (1,28,28) â†’ (28,28)\n",
        "    label = labels[i].item()\n",
        "\n",
        "    axes[i].imshow(image, cmap='gray')\n",
        "    axes[i].set_title(classes[label])\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "8verH9OxW7KK",
        "outputId": "bd0cd812-c1d3-4e86-e1ca-8072d353f9fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2925931912>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFashionMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Get a batch of images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2Q6bvXWFoo2",
        "outputId": "52e6a23c-a700-4914-d565-6a893240ac3b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Lenet(\n",
              "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (activate1): ReLU()\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (activate2): ReLU()\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv3): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (activate3): ReLU()\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (linear1): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (activate4): ReLU()\n",
              "  (linear2): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Lenet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Lenet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
        "        self.activate1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
        "        self.activate2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1)\n",
        "        self.activate3 = nn.ReLU()\n",
        "\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear1 = nn.Linear(in_features=120, out_features=84)\n",
        "        self.activate4 = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(in_features=84, out_features=num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.pad(x, (2, 2, 2, 2))\n",
        "        x = self.conv1(x)\n",
        "        x = self.activate1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x=self.conv2(x)\n",
        "        x=self.activate2(x)\n",
        "        x=self.pool2(x)\n",
        "\n",
        "        x=self.conv3(x)\n",
        "        x=self.activate3(x)\n",
        "\n",
        "        x=self.flatten(x)\n",
        "        x=self.linear1(x)\n",
        "        x=self.activate4(x)\n",
        "        x=self.linear2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu' )\n",
        "\n",
        "model=Lenet(10).to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "355e552f",
        "outputId": "0d0def42-a029-423b-9a47-d193081aa1f4"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Convert NumPy arrays to PyTorch tensors\n",
        "x_train_tensor = torch.tensor(x_train, dtype=torch.float32).unsqueeze(1) / 255.0\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "x_val_tensor = torch.tensor(x_test, dtype=torch.float32).unsqueeze(1) / 255.0\n",
        "y_val_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "x_test_tensor = torch.tensor(x_test, dtype=torch.float32).unsqueeze(1) / 255.0\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "\n",
        "val_size=int(0.1*len(train_dataset))\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"Data preprocessing complete. DataLoaders created.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preprocessing complete. DataLoaders created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#coding the training loop\n",
        "import torch.optim as optim\n",
        "\n",
        "current_loss=0\n",
        "\n",
        "\n",
        "loss_calc=nn.CrossEntropyLoss()\n",
        "optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
        "\n",
        "num_epochs=5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    current_loss=0\n",
        "    for images,label in train_loader:\n",
        "        images,label=images.to(device),label.to(device)\n",
        "\n",
        "          #clearing past gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #forward pass\n",
        "\n",
        "        output=model(images)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #loss\n",
        "        loss=loss_calc(output,label)\n",
        "\n",
        "\n",
        "\n",
        "        #loss back propagation\n",
        "        loss.backward()\n",
        "\n",
        "         #update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        current_loss+=loss.item()* images.size(0)\n",
        "\n",
        "\n",
        "    # print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss/len(train_loader):.4f}\")\n",
        "    avg_train_loss = current_loss / len(train_loader.dataset)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, label in val_loader:\n",
        "            images, label = images.to(device), label.to(device)\n",
        "            output = model(images)\n",
        "            loss = loss_calc(output, label)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
        "          f\"Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4Q2e9_0KSXg",
        "outputId": "0ce72676-ceb6-4821-fcf8-1d41a85db89f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Train Loss: 0.0440, Val Loss: 0.0449\n",
            "Epoch [2/5], Train Loss: 0.0401, Val Loss: 0.0362\n",
            "Epoch [3/5], Train Loss: 0.0476, Val Loss: 0.0277\n",
            "Epoch [4/5], Train Loss: 0.0357, Val Loss: 0.0409\n",
            "Epoch [5/5], Train Loss: 0.0374, Val Loss: 0.0362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # Set to evaluation mode\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():  # No need to track gradients\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Test Accuracy: {100 * correct / total:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlSuaDXNSWh5",
        "outputId": "634463a5-d766-47eb-c799-02a1ea3b0289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 90.01%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing model on real photo\n",
        "\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "\n",
        "# Define transform (must match your training preprocessing)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),  # Ensure grayscale\n",
        "    transforms.Resize((28, 28)),                  # Resize to FashionMNIST size\n",
        "    transforms.ToTensor(),                        # Convert to tensor\n",
        "    transforms.Normalize((0.5,), (0.5,))          # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "# Load the image\n",
        "image_path = \"v.png\"  # Change this to your actual image path\n",
        "img = Image.open(image_path)\n",
        "\n",
        "# Apply the transform\n",
        "img_tensor = transform(img).unsqueeze(0)  # Shape: [1, 1, 28, 28]\n",
        "\n",
        "################# View Image\n",
        "img_unnorm = img_tensor * 0.5 + 0.5  # reverse normalization\n",
        "\n",
        "# Convert to numpy, squeeze channel dimension\n",
        "img_np = img_unnorm.squeeze().cpu().numpy()\n",
        "\n",
        "# Plot\n",
        "plt.imshow(img_np, cmap='gray')\n",
        "plt.title(\"Transformed Image (unnormalized)\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Move to the same device as model\n",
        "img_tensor = img_tensor.to(device)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Make prediction\n",
        "with torch.no_grad():\n",
        "    output = model(img_tensor)\n",
        "    predicted_class = output.argmax(dim=1).item()\n",
        "\n",
        "print(predicted_class)\n",
        "print(f\"Predicted class:\",classes[predicted_class])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "PLYbKooJWnx1",
        "outputId": "552b1ea1-b2c5-4e5c-9a62-0f38e584d91e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "Predicted class: Sandal\n"
          ]
        }
      ]
    }
  ]
}